# ============================================================================
# Multi-Stage Dockerfile for STT Service
# Optimized for high-end RTX graphics cards (3090, 4090, 5090)
# ============================================================================

# ============================================================================
# Stage 1: Builder - Install dependencies and models
# ============================================================================
FROM nvidia/cuda:12.3.2-cudnn9-devel-ubuntu22.04 AS builder

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-dev \
    python3-pip \
    build-essential \
    cmake \
    pkg-config \
    wget \
    curl \
    git \
    portaudio19-dev \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set up Python 3.10 as default python3
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1

# Upgrade pip
RUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download models during build to reduce startup time
ARG WHISPER_MODEL=large-v3
ENV WHISPER_MODEL=${WHISPER_MODEL}
RUN python3 -c "from faster_whisper import WhisperModel; WhisperModel('${WHISPER_MODEL}', device='cpu', download_root='/app/models')" || echo "Model pre-download completed"

# ============================================================================
# Stage 2: Runtime - Minimal production image
# ============================================================================
FROM nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install only runtime dependencies (minimal footprint)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3-pip \
    # Audio dependencies (runtime only)
    portaudio19-dev \
    libasound2 \
    # Media processing (runtime only)
    ffmpeg \
    libavcodec58 \
    libavformat58 \
    libavutil56 \
    # Health check utility
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set up Python 3.10 as default python3
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1

# Set working directory
WORKDIR /app

# Copy Python packages from builder
COPY --from=builder /usr/local/lib/python3.10/dist-packages /usr/local/lib/python3.10/dist-packages

# Copy pre-downloaded models from builder
COPY --from=builder /app/models /app/models

# Copy application code
COPY app ./app
COPY main.py .

# Create directories for logs and ensure proper permissions
RUN mkdir -p /app/logs && \
    chmod 755 /app/logs

# ============================================================================
# Environment Variables - Production Optimized
# ============================================================================

# Model and storage configuration (can be overridden via docker-compose)
ENV MODEL_PATH=/app/models \
    WHISPER_BACKEND=faster-whisper \
    WHISPER_MODEL=large-v3 \
    LOG_LEVEL=INFO

# GPU device selection (can be overridden)
ENV CUDA_VISIBLE_DEVICES=0 \
    NVIDIA_VISIBLE_DEVICES=0 \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# CUDA optimizations for high-end RTX cards
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID \
    CUDA_LAUNCH_BLOCKING=0 \
    CUDA_CACHE_DISABLE=0 \
    CUDA_MEMORY_FRACTION=0.95

# cuDNN and CUDA library paths
ENV LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:${LD_LIBRARY_PATH}" \
    CUDA_HOME="/usr/local/cuda" \
    PATH="/usr/local/cuda/bin:${PATH}"

# PyTorch optimizations for high-end RTX cards
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
    TORCH_CUDNN_V8_API_ENABLED=1

# Application configuration
ENV PYTHONPATH=/app \
    PYTHONUNBUFFERED=1

# WebSocket server configuration (can be overridden)
ENV STT_HOST=0.0.0.0 \
    STT_PORT=8000 \
    STT_MAX_CONNECTIONS=50

# Create non-root user for security
RUN useradd -m -u 1000 sttuser && \
    chown -R sttuser:sttuser /app

# Switch to non-root user
USER sttuser

# Health check with improved WebSocket endpoint testing
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -f http://localhost:${STT_PORT}/health || exit 1

# Expose ports (WebSocket server on 8000, monitoring on 9091)
EXPOSE 8000 9091

# Default command - WebSocket server mode with environment variables
CMD python3 main.py websocket \
    --host ${STT_HOST} \
    --port ${STT_PORT} \
    --max-connections ${STT_MAX_CONNECTIONS} \
    --log-level ${LOG_LEVEL}